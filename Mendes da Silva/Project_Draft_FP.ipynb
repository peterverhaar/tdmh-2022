{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca1fd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize , sent_tokenize , pos_tag\n",
    "from tdmh import *\n",
    "from os.path import join\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5905f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\RS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\RS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8fb0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "dir = r'C:\\Users\\RS\\Desktop\\FP_T&DM\\Corpus'\n",
    "\n",
    "for entry in os.listdir(dir):\n",
    "    # dir + subdirectory\n",
    "    path = os.path.join(dir,entry)\n",
    "    if os.path.isdir(path):\n",
    "        \n",
    "        for file in os.listdir(path):\n",
    "            # dir + subdirectory + file \n",
    "            file_path = join(path,file)\n",
    "            texts.append( file_path )\n",
    "            print( file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3b9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a16bb0053846f1a134f5bdddb95697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 00:09:26 INFO: Downloading default packages for language: pt (Portuguese)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2825a1c3d4a4f9faaab3f09b8a09bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.3.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 00:10:58 INFO: Finished downloading models and saved to C:\\Users\\RS\\stanza_resources.\n",
      "2022-04-06 00:10:58 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "| depparse  | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-04-06 00:10:58 INFO: Use device: cpu\n",
      "2022-04-06 00:10:58 INFO: Loading: tokenize\n",
      "2022-04-06 00:10:59 INFO: Loading: mwt\n",
      "2022-04-06 00:10:59 INFO: Loading: pos\n",
      "2022-04-06 00:10:59 INFO: Loading: lemma\n",
      "2022-04-06 00:10:59 INFO: Loading: depparse\n",
      "2022-04-06 00:11:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('pt')       \n",
    "nlp = stanza.Pipeline('pt')\n",
    "\n",
    "def get_title(path):\n",
    "    title = os.path.basename(path)\n",
    "    if re.search( r'txt$' , title ):\n",
    "        # Remove txt extension\n",
    "        title = title[ :title.index('.txt') ]\n",
    "        # remove commas and dots\n",
    "        title = re.sub( r'[.,]' , '' , title )\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca720ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O poeta é um fingidor\n",
      "Finge tão completamente\n",
      "Que chega a fingir que é dor\n",
      "A dor que deveras sente.\n",
      "\n",
      "E os que lêem o que escreve,\n",
      "Na dor lida sentem bem,\n",
      "Não as duas que ele teve,\n",
      "Mas só a que eles não têm.\n",
      "\n",
      "E assim nas calhas de roda\n",
      "Gira, a entreter a razão,\n",
      "Esse comboio de corda\n",
      "Que se chama coração.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open( 'Corpus\\FP_Autopsicografia.txt' , encoding = 'utf-8') as file:\n",
    "    full_text = file.read() \n",
    "    \n",
    "print(full_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a465d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the words in this poem: ['O', 'poeta', 'é', 'um', 'fingidor', 'Finge', 'tão', 'completamente', 'Que', 'chega', 'a', 'fingir', 'que', 'é', 'dor', 'A', 'dor', 'que', 'deveras', 'sente', 'E', 'os', 'que', 'lêem', 'o', 'que', 'escreve', 'Na', 'dor', 'lida', 'sentem', 'bem', 'Não', 'as', 'duas', 'que', 'ele', 'teve', 'Mas', 'só', 'a', 'que', 'eles', 'não', 'têm', 'E', 'assim', 'nas', 'calhas', 'de', 'roda', 'Gira', 'a', 'entreter', 'a', 'razão', 'Esse', 'comboio', 'de', 'corda', 'Que', 'se', 'chama', 'coração'].\n",
      "There are 64 words in this poem.\n",
      "There is 305 characters in this poem.\n"
     ]
    }
   ],
   "source": [
    "import tdmh\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "    \n",
    "with open( 'Corpus\\FP_Autopsicografia.txt' , encoding = 'utf-8') as poem:\n",
    "    full_text = poem.read()\n",
    "\n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "print(f'These are the words in this poem: {words}.')\n",
    "print(f'There are { len(words) } tokens in this poem.')\n",
    "print(f'There is {len(full_text)} characters in this poem.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
